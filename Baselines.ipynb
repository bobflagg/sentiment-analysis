{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some Baselines for Sentiment Analysis\n",
    "\n",
    "A good starting point for understanding recent work in sentiment analysis and text classification is \n",
    "[_Baselines and Bigrams: Simple, Good Sentiment and Topic Classification_](http://nlp.stanford.edu/pubs/sidaw12_simple_sentiment.pdf) by Sida Wang and Christopher D. Manning. In this notebook, I'll implement the models described in that paper and try to reproduce their results on several datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "| AthR  | XGraph | BbCrypt|   CR   |  IMDB  | MPQA   | RT-2k  | RTs    | subj   |              |\n",
    "|-------|:------:|:------:|:------:|:------:|:------:|:------:|:------:|:------:|-------------:|\n",
    "| 85.13 |  91.19 |  99.40 |  79.97 |  86.59 |  86.27 |  85.85 |  79.03 |  93.56 |  MNB-bigram  |\n",
    "| 84.99 |  89.96 |  99.29 |  79.76 |  83.55 |  85.29 |  83.45 |  77.94 |  92.58 |  MNB-unigram | \n",
    "| 83.73 |  86.17 |  97.68 |  80.85 |  89.16 |  86.72 |  87.40 |  77.72 |  91.74 |  SVM-bigram  | \n",
    "| 82.61 |  85.14 |  98.29 |  79.02 |  86.95 |  86.15 |  86.25 |  76.23 |  90.84 |  SVM-unigram |  \n",
    "| 87.66 |  90.68 |  99.50 |  81.75 |  91.22 |  86.32 |  89.45 |  79.38 |  93.18 |  NBSVM-bigram|  \n",
    "| 87.94 |  91.19 |  99.70 |  80.45 |  88.29 |  85.25 |  87.80 |  78.05 |  92.40 |  SVM-unigram |\n",
    "\n",
    "[peng](http://nlp.stanford.edu/wiki/Software/Classifier/Sentiment)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Datasets\n",
    "\n",
    "The baselines and bigrams paper uses several standard datasets to run sentiment analysis experiments. \n",
    "In this section I'll show how to prepare these datasets for training and evaluating classifiers.\n",
    "\n",
    "### RT-s\n",
    "\n",
    "The dataset consists of 2,000 full-length movie reviews and was introducted in \n",
    "[Pang and Lee, 2004](http://www.aclweb.org/anthology/P04-1035).\n",
    "\n",
    "### RT-2k\n",
    "\n",
    "The dataset consists of 2,000 full-length movie reviews and was introducted in \n",
    "[Pang and Lee, 2004](http://www.aclweb.org/anthology/P04-1035).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMDB\n",
    "\n",
    "A large movie review dataset with 50K full-length reviews [Maas et al., 2011](http://ai.stanford.edu/ ∼ amaas/data/sentiment).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "imdb_df = pd.read_csv('/home/data/sentiment-analysis-and-text-classification/baselines-and-bigrams/aclImdb/data-frame.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "imdb_X_train = imdb_df.loc[:25000, 'review'].values\n",
    "imdb_y_train = imdb_df.loc[:25000, 'sentiment'].values\n",
    "imdb_X_test = imdb_df.loc[25000:, 'review'].values\n",
    "imdb_y_test = imdb_df.loc[25000:, 'sentiment'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial Naive Bayes (MNB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imdb_clf = Pipeline([\n",
    "    ('vect', CountVectorizer(ngram_range=(1,2))),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultinomialNB())\n",
    "])\n",
    "_ = imdb_clf.fit(imdb_X_train, imdb_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.87187999999999999"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted = imdb_clf.predict(imdb_X_test)\n",
    "np.mean(predicted == imdb_y_test)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.84      0.91      0.88     12500\n",
      "        pos       0.91      0.83      0.87     12500\n",
      "\n",
      "avg / total       0.87      0.87      0.87     25000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[11425,  1075],\n",
       "       [ 2128, 10372]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(metrics.classification_report(y_test, predicted, target_names=['neg', 'pos']))\n",
    "metrics.confusion_matrix(imdb_y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NBSVM\n",
    "\n",
    "There are several implementations of NBSVM available; for example:\n",
    "\n",
    "* Sida Wang's original [implementation](https://github.com/sidaw/nbsvm) in Matlab.  \n",
    "* A Python [version](https://github.com/mesnilgr/nbsvm) by Grégoire Mesnil.  \n",
    "* Daniel Pressel's [version](https://github.com/dpressel/nbsvm-xl) in Java.  \n",
    "\n",
    "I'll follow the elegant [implementation](https://github.com/Joshua-Chin/nbsvm) in scikit-learn by Joshua Chin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import spmatrix, coo_matrix\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.linear_model.base import LinearClassifierMixin, SparseCoefMixin\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NBSVM(BaseEstimator, LinearClassifierMixin, SparseCoefMixin):\n",
    "\n",
    "    def __init__(self, alpha=1, C=1, beta=0.25, fit_intercept=False):\n",
    "        self.alpha = alpha\n",
    "        self.C = C\n",
    "        self.beta = beta\n",
    "        self.fit_intercept = fit_intercept\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.classes_ = np.unique(y)\n",
    "        if len(self.classes_) == 2:\n",
    "            coef_, intercept_ = self._fit_binary(X, y)\n",
    "            self.coef_ = coef_\n",
    "            self.intercept_ = intercept_\n",
    "        else:\n",
    "            coef_, intercept_ = zip(*[\n",
    "                self._fit_binary(X, y == class_)\n",
    "                for class_ in self.classes_\n",
    "            ])\n",
    "            self.coef_ = np.concatenate(coef_)\n",
    "            self.intercept_ = np.array(intercept_).flatten()\n",
    "        return self\n",
    "\n",
    "    def _fit_binary(self, X, y):\n",
    "        p = np.asarray(self.alpha + X[y == 1].sum(axis=0)).flatten()\n",
    "        q = np.asarray(self.alpha + X[y == 0].sum(axis=0)).flatten()\n",
    "        r = np.log(p/np.abs(p).sum()) - np.log(q/np.abs(q).sum())\n",
    "        b = np.log((y == 1).sum()) - np.log((y == 0).sum())\n",
    "\n",
    "        if isinstance(X, spmatrix):\n",
    "            indices = np.arange(len(r))\n",
    "            r_sparse = coo_matrix(\n",
    "                (r, (indices, indices)),\n",
    "                shape=(len(r), len(r))\n",
    "            )\n",
    "            X_scaled = X * r_sparse\n",
    "        else:\n",
    "            X_scaled = X * r\n",
    "\n",
    "        lsvc = LinearSVC(\n",
    "            C=self.C,\n",
    "            fit_intercept=self.fit_intercept,\n",
    "            max_iter=10000\n",
    "        ).fit(X_scaled, y)\n",
    "\n",
    "        mean_mag =  np.abs(lsvc.coef_).mean()\n",
    "        coef_ = (1 - self.beta) * mean_mag * r + self.beta * (r * lsvc.coef_)\n",
    "        intercept_ = (1 - self.beta) * mean_mag * b + self.beta * lsvc.intercept_\n",
    "\n",
    "        return coef_, intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "imdb_nbsvm = Pipeline([\n",
    "    ('vect', CountVectorizer(),\n",
    "    ('clf', NBSVM()\n",
    "])\n",
    "_ = imdb_nbsvm.fit(imdb_X_train, imdb_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88112000000000001"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted = imdb_nbsvm.predict(imdb_X_test)\n",
    "np.mean(predicted == imdb_y_test)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.87      0.89      0.88     12500\n",
      "        pos       0.89      0.87      0.88     12500\n",
      "\n",
      "avg / total       0.88      0.88      0.88     25000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[11133,  1367],\n",
       "       [ 1605, 10895]])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(metrics.classification_report(imdb_y_test, predicted, target_names=['neg', 'pos']))\n",
    "metrics.confusion_matrix(imdb_y_test, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from scipy.stats import uniform\n",
    "imdb_nbsvm = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('clf', NBSVM())\n",
    "])\n",
    "param_distributions = {\n",
    "    'vect__ngram_range': [(1,2), (1,3)],\n",
    "    'vect__stop_words': [None],\n",
    "    'clf__beta': uniform(0, 1),\n",
    "    'clf__C': [1.0]\n",
    "}\n",
    "rsearch = RandomizedSearchCV(estimator=imdb_nbsvm, param_distributions=param_distributions, n_iter=25)\n",
    "rsearch.fit(imdb_X_train, imdb_y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.914963401464\n",
      "{'clf__beta': 0.53300408355730355, 'clf__C': 33.83938547529678, 'vect__stop_words': None, 'vect__ngram_range': (1, 2)}\n"
     ]
    }
   ],
   "source": [
    "#print(rsearch)\n",
    "# summarize the results of the grid search\n",
    "print(rsearch.best_score_)\n",
    "print(rsearch.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "imdb_nbsvm = Pipeline([\n",
    "    ('vect', CountVectorizer(ngram_range=(1, 3))),\n",
    "    ('clf', NBSVM(C=1.0, beta=0.53300408355730355))\n",
    "])\n",
    "imdb_nbsvm.fit(imdb_X_train, imdb_y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predicted = imdb_nbsvm.predict(imdb_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.90247999999999995"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(predicted == imdb_y_test)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.90      0.91      0.90     12500\n",
      "        pos       0.91      0.90      0.90     12500\n",
      "\n",
      "avg / total       0.90      0.90      0.90     25000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[11347,  1153],\n",
       "       [ 1285, 11215]])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(metrics.classification_report(imdb_y_test, predicted, target_names=['neg', 'pos']))\n",
    "metrics.confusion_matrix(y_test, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prepare a uniform distribution to sample for the alpha parameter\n",
    "param_grid = {'alpha': sp_rand()}\n",
    "# create and fit a ridge regression model, testing random alpha values\n",
    "model = Ridge()\n",
    "rsearch = RandomizedSearchCV(estimator=model, param_distributions=param_grid, n_iter=100)\n",
    "rsearch.fit(dataset.data, dataset.target)\n",
    "print(rsearch)\n",
    "# summarize the results of the random parameter search\n",
    "print(rsearch.best_score_)\n",
    "print(rsearch.best_estimator_.alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imdb_nbsvm = Pipeline([\n",
    "    ('vect', CountVectorizer(ngram_range=(1,2), stop_words='english')),\n",
    "    ('clf', NBSVM( C=0.3, beta=0.5))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p = np.asarray(1.0 + X_train[y_train == 1].sum(axis=0)).flatten()\n",
    "q = np.asarray(1.0 + X_train[y_train == 0].sum(axis=0)).flatten()\n",
    "r = np.log(p/np.abs(p).sum()) - np.log(q/np.abs(q).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 4996192)\n",
      "p (4996192,)\n",
      "q (4996192,)\n",
      "r (4996192,)\n",
      "4996192\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(\"p\", p.shape)\n",
    "print(\"q\", q.shape)\n",
    "print(\"r\", r.shape)\n",
    "indices = np.arange(len(r))\n",
    "print(len(indices))\n",
    "indices[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 1, 1)\n",
      "([1, 2, 3], [1, 2, 3], [1, 2, 3], [1, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "for x in zip(*[(1,[1,2,3]), (1,[1,2,3]), (1,[1,2,3]), (1,[1,2,3])]): print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def load_imdb(data_directory='/home/data/sentiment-analysis-and-text-classification/baselines-and-bigrams/aclImdb'):\n",
    "    print(\"Vectorizing Training Text\")\n",
    "    \n",
    "    train_pos = glob.glob(os.path.join(data_directory, 'train', 'pos', '*.txt'))\n",
    "    train_neg = glob.glob(os.path.join(data_directory, 'train', 'neg', '*.txt'))\n",
    "\n",
    "    token_pattern = r'\\w+|[%s]' % string.punctuation\n",
    "\n",
    "    vectorizer = CountVectorizer(\n",
    "        'filename', \n",
    "        ngram_range=(1, 3),\n",
    "        token_pattern=token_pattern,\n",
    "        binary=True\n",
    "    )\n",
    "    X_train = vectorizer.fit_transform(train_pos+train_neg)\n",
    "    y_train = np.array([1]*len(train_pos)+[0]*len(train_neg))\n",
    "\n",
    "    print(\"Vocabulary Size: %s\" % len(vectorizer.vocabulary_))\n",
    "    print(\"Vectorizing Testing Text\")\n",
    "\n",
    "    test_pos = glob.glob(os.path.join(data_directory, 'test', 'pos', '*.txt'))\n",
    "    test_neg = glob.glob(os.path.join(data_directory, 'test', 'neg', '*.txt'))\n",
    "\n",
    "    X_test = vectorizer.transform(test_pos + test_neg)\n",
    "    y_test = np.array([1]*len(test_pos)+[0]*len(test_neg))\n",
    "\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing Training Text\n",
      "Vocabulary Size: 4996192\n",
      "Vectorizing Testing Text\n",
      "CPU times: user 1min 27s, sys: 4.31 s, total: 1min 32s\n",
      "Wall time: 4min 9s\n"
     ]
    }
   ],
   "source": [
    "%time X_train, y_train, X_test, y_test = load_imdb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 4996192) (25000,)\n",
      "(25000, 4996192) (25000,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NBSVM(C=1, alpha=1, beta=0.25, fit_intercept=False)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnbsvm = NBSVM()\n",
    "mnbsvm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.92032\n"
     ]
    }
   ],
   "source": [
    "print('Test Accuracy: %s' % mnbsvm.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x4996192 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 790 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9209261336324307"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "11495 / (11495 + 987)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "       1, 1, 1, 1, 0, 1, 1])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[-30:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
